from __future__ import annotations
import json
from pathlib import Path
import argparse

def load_metrics(p: Path):
    with open(p, "r", encoding="utf-8") as f:
        return json.load(f)

def row(name: str, m: dict) -> str:
    t = m["test"]
    return f"{name} & {t['accuracy']:.3f} & {t['precision']:.3f} & {t['recall']:.3f} & {t['f1']:.3f} \\"

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--results_dir", default="results")
    ap.add_argument("--out", default="results/results_table.tex")
    args = ap.parse_args()

    rdir = Path(args.results_dir)
    outp = Path(args.out)
    outp.parent.mkdir(parents=True, exist_ok=True)

    # Expected metric files (run training scripts first)
    paths = [
        ("LogReg", rdir / "baseline_logreg_none_metrics.json"),
        ("LogReg + class\_weight", rdir / "baseline_logreg_balanced_metrics.json"),
        ("MLP", rdir / "mlp_none_metrics.json"),
        ("MLP + pos\_weight", rdir / "mlp_pos_weight_metrics.json"),
    ]

    rows = []
    for name, p in paths:
        if p.exists():
            rows.append(row(name, load_metrics(p)))
        else:
            rows.append(f"{name} & -- & -- & -- & -- \\ % missing: {p.name}")

    tex = "\n".join([
        "% Auto-generated by scripts/make_results_table.py",
        "\\begin{table}[t]",
        "\\centering",
        "\\begin{tabular}{lcccc}",
        "\\hline",
        "Model & Acc & Prec & Recall & F1 \\",
        "\\hline",
        *rows,
        "\\hline",
        "\\end{tabular}",
        "\\caption{Performance on NSL-KDD (attack is positive class).}",
        "\\label{tab:results}",
        "\\end{table}",
        ""
    ])

    outp.write_text(tex, encoding="utf-8")
    print("Wrote:", outp)

if __name__ == "__main__":
    main()
